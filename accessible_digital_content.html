<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>lotus - Accessible Digital Content Creation</title>
  <meta name="description" content="">
  <meta name="author" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="icon" type="image/png" href="images/favicon.png">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <!-- <script src="audio.js" defer></script> -->
</head>
<body>

  <div class="outer-container-0">

    <div class="container">
      <div style="height: 25px"></div>

      <div class="row small">
        <div class="one-half column">
          <!-- <h5>Research + Design</h5> -->
          <h2><strong><a href="index.html">LOTUS</strong> ZHANG</a></h2>
        </div>
        <div class="one-half column">
          <a class="button right" href="lotus-resume.pdf">resume</a>
        </div>
      </div>
    </div>
  </div>
  <div class="outer-container-1">
    <div class="container">

      <div style="height: 50px"></div>

      <h2>Accessible Digital Content Creation</h2>
      <p>
        <b>Role:</b> PhD Researcher, University of Washington (HCDE)<br>
        <b>Methods:</b> Surveys, interviews, participatory design, prototyping, AI-assisted tool evaluation<br>
        <b>Impact:</b> Produced design insights and prototypes influencing accessibility research and future creative platforms<br>
      </p>
      <br>

      <h4>Overview</h4>
      <p>Most digital creative tools, from presentation software to photo editors, assume visual fluency, leaving Blind and low-vision (BLV) individuals excluded from meaningful creative expression. Over my PhD, I led a <b>multi-phase research agenda</b> to uncover BLV creators’ needs, test new interaction paradigms, and design prototypes that expand access.<br>
      Across three studies, I identified <b>unmet needs in creativity support</b>, validated approaches through user studies, and generated insights that could guide <b>industry design of inclusive creative tools</b>.</p>
      <br>

      <h4>The Challenge</h4>
      <ul>
      <li><b>For users:</b> BLV creators want to engage in diverse creative practices, but current tools provide little to no feedback beyond “alt text,” limiting both functionality and self-expression.</li>
      <li><b>For companies:</b> Creative platforms risk excluding millions of users if workflows remain vision-centric. This exclusion reduces adoption, trust, and brand equity.</li>
      <li><b>For design teams:</b> Existing accessibility work is fragmented (e.g., photo capture or code editing), with little understanding of BLV creative workflows end-to-end.</li>
      </ul>
      <br>

      <h4>My Approach</h4>
      <p>I structured the work into three phases:
      <ol>
        <li><b>Formative Study - Understanding BLV Creativity Needs</b> <a href="https://dl.acm.org/doi/10.1145/3597638.3608387"><u>[ASSETS 2023 Best Paper Nominee]</u></a>
          <ul>
            <li><b>Unmet Need:</b> Companies often assume BLV users aren’t interested in visual creation. I identified broad interest in visual and expressive content, from slides to social media.</li>
            <li><b>Method:</b> Surveyed 165 BLV individuals and conducted 15 in-depth interviews.</li>
            <li><b>Insight for Industry:</b> BLV creators are not a “niche”; they want to participate fully in visual creative work. Platforms should design with them in mind rather than assuming exclusion.</li>
          </ul>
        </li>

        <img src="images/fig2.png" width="90%" style="display: block; margin: 0 auto; border: 10px solid white;" alt='Two bar charts compare blind and low-vision participants’ interest and experience in creating text-based, audio, static visual, video, and interactive content. Both groups show the highest interest and experience in text-based and audio content, while fewer report experience with interactive content. Blind participants report high interest and experience with text and audio, but also show notable interest in creating static visual and video content. Low-vision participants show slightly higher engagement with static visual and video content compared to blind participants.'>
        <br>
        <div style="text-align: center; font-size: 0.9em;"><i>Percentages of respondents with experience and interest in five general content creation types across visual conditions, sorted in descending order. In summary, blind and low vision people's interest in creating digital content did not vary greatly across content types or vision conditions, but their actual creation experience centered on text-based and audio content, especially for blind people. Error bars are 95% confidence intervals.</i></div>
        <br>

        <li><b>Privacy in Photo Editing – Agency in Obfuscation</b> <a href="https://dl.acm.org/doi/10.1145/3613904.3642713"><u>[CHI 2024]</u></a>
          <ul>
            <li><b>Unmet Need:</b> BLV users struggle to share images privately without sighted help. Current tools either over-automate (removing agency) or under-support (requiring sight).</li>
            <li><b>Method:</b> Co-designed and evaluated a screen-reader-accessible obfuscation tool with 12 BLV participants, testing AI-assisted vs. Wizard-of-Oz prototypes.</li>
            <li><b>Insight for Industry:</b> Privacy tools must go beyond “black box automation.” Users need <b>transparent, explainable edits</b> and flexible levels of control. This principle extends to broader AI tool design (trust = explainability + agency).</li>
          </ul>
        </li>

        <img src="images/fig3.jpg" width="90%" style="display: block; margin: 0 auto; border: 10px solid white;" alt='This prototype user interface illustration contains three screens, with the left one displaying the explore image section within a photo obfuscation page. This section has a photo, a “Explore what’s in the image through touch” button under that photo, and a call-out with text: “High-level caption: Three mangoes on a white paper in the middle of a wood table. The white paper shows several lines of small, blurry text.” The middle screen displays a touch-based explorer interface, with the same photo but with multiple bounding boxes surrounding key objects, which were described in call-outs: (1) “a wood table in the background”, (2) “a plastic bag”, (3)“A paper document containing small blurry texts under three mangoes. Texts: Gamora Zen, 8551 Hilldale Dr...”, (4) “Three yellow mangoes on top of a paper document”. Finally, the right screen displays the same photo obfuscation page, with scrolled down to the edit image section, which contains three options: (1)item down-down menu, with paper document containing small blurry text selected, (2) style drop-down menu, with blur selected, and (3) shape drop-down menu, with exact selected. There are also an apply edit button, a revert button, and a complete and submit button on the screen.'>
        <br>
        <div style="text-align: center; font-size: 0.9em;"><i>Prototype design for an accessible visual privacy management app: (1) explore image section, including a high-level caption (left) and a touch-based explorer with captions displayed for each object bounding box (middle), (2) edit image section (right). The prototype allows users to explore the image and obfuscate private objects by choosing from a list of all potential private objects detected by the system.</i></div>
        <br>

        <li><b>Expressive Visual Editing – Beyond Functionality to Aesthetics</b> [ASSETS 2025 To Appear]
          <ul>
            <li><b>Unmet Need:</b> BLV creators don’t just want to complete tasks - they want to express identity through aesthetics (color, mood, style), yet tools ignore expressive goals.</li>
            <li><b>Method:</b> Conducted 10 interviews + a design probe study (14 participants) with VizXpress, an AI-powered prototype that offered aesthetic feedback, suggestions, and accessible manual editing.</li>
            <li><b>Insight for Industry:</b> Expressive creativity is possible if tools provide <b>layered, comparative, and goal-aligned feedback</b>. Generative AI should scaffold expression, not replace it - giving users control over mood, style, and creative direction.</li>
          </ul>
        </li>

        <img src="images/fig4.png" width="90%" style="display: block; margin: 0 auto; border: 10px solid white;" alt='The mobile interface showcases the feedback section of VizXpress, consisting of an image of a dog sitting on a carpeted floor near a fluffy chair and cables, followed by a button labeled with download the current image, and then five feedback mechanisms: (1) a high-level alt text (A dog sitting on a carpeted floor near a fluffy chair and cables), (2) object information (key elements in the image: dog, carpet, fluffy chair, and power cables), (3) aesthetics evaluation (visual quality, noting dim lighting, muted colors, cluttered composition, and a cozy but casual candid style), (4) improvement suggestions (recommends increasing brightness, cropping out wires, and boosting contrast to enhance clarity and focus on the dog), and (5) question and answer.'>
        <br>
        <div style="text-align: center; font-size: 0.9em;"><i>The real-time visual feedback design of VizXpress prototype for accessible visual aesthetics editing. The section provides (1) a high-level alt text; (2) object information; (3) aesthetics evaluation; (4) suggestions; (5) question and answer mechanism.</i></div>
        <br>

        <img src="images/fig5.png" width="90%" style="display: block; margin: 0 auto; border: 10px solid white;" alt='The mobile interface showcases the make edit section of VizXpress, which contains six edit options: (1) edits from recommendation function that allows the user to input a high-level requirement to request recommendations and to apply the recommended edits. (An example recommendation is "increase the brightness to make the image clearer and more vibrant. Adjust the brightness to a level of 30 for better visibility of the dog and surrounding details".) (2) color and lighting function that shows the current brightness, contrast, and saturation level (e.g., current brightness level is 10) and includes an increase, decrease, and reset button to each visual quality. (3) filter function, that shows the current selected filter, and eight filter options: fresco, bali, nordic, chroma, aura, antiq, noir, and outrun, each labeled with descriptive texts (e.g., Noir turns your photo into a black and white image, emphasizing contrasts and shadows.) (4) crop by objects function, that shows key objects from the original image (e.g., a dog, a furry chair, a power strip) and options to select one or multiple of these objects and the amount of background surrounding these objects to keep in the cropped image. (5) textbox function, where the user follows a step-by-step process: enter the text (e.g., cute Frenchie), select a font style (e.g., Comic Sans MS: casual and playful), choose a color (e.g., white, clean and highly readable), and set a background (e.g., transparent background). For stickers, users input simple keywords (e.g., red heart). For both text and stickers, users can choose between manual positioning (specifying x, y, and width) or automatic positioning by describing where they should appear.'>
        <br>
        <div style="text-align: center; font-size: 0.9em;"><i>The accessible editing interaction design of VizXpress prototype. The interface includes automated ((1) Edits from Recommendations) and manual ((2) Color and Lighting, (3) Filter, (4) Crop, (5) Text, and (6) Sticker) editing options.</i></div>

      </ol>

      <br>

      <h4>Key Cross-Phase Insights</h4>
      Across studies, I found that:
      <ul>
        <li><b>Creativity is universal:</b> BLV users want to create visual content, not just consume or “be accommodated.”</li>
        <li><b>Feedback is the bottleneck:</b> Accessible tools must provide layered, contextual feedback that adapts to user goals (e.g., privacy vs. aesthetics).</li>
        <li><b>AI must be transparent and collaborative:</b> Blind creators distrust opaque automation; effective tools provide <b>scaffolding + agency.</b></li>
        <li><b>Inclusive design benefits everyone:</b> Transparent obfuscation tools and better aesthetic guidance can also help sighted users (e.g., managing privacy, editing efficiently).</li>
      </ul>
      <br>

      <h4>Impact</h4>
      <ul>
        <li><b>For Accessibility Research:</b> Publications at CHI and ASSETS advanced knowledge of BLV creative practices.</li>
        <li><b>For Industry:</b> Produced prototypes and design principles that can guide companies building creative AI and editing platforms.
          <ul>
            <li>Privacy obfuscation → applicable to photo-sharing tools (Instagram, Google Photos).</li>
            <li>Expressive editing → applicable to AI editing assistants, social platforms, and creative software.</li>
          </ul>
        </li>
        <li><b>For Design Practice:</b> Demonstrated that <i>reframing accessibility from “accommodation” to “expression”</i> opens new product opportunities.</li>
      </ul>
      <br>

      <h4>Reflection</h4>
      Through this project, I learned how to:
      <ul>
        <li>Scope multi-phase research to move from broad needs → focused prototypes → design principles.</li>
        <li>Balance rigor and feasibility (AI’s capabilities vs. its trust challenges).</li>
        <li>Translate nuanced findings into insights product teams can act on immediately.</li>
      </ul>

    </div>
  </div>


  <div id="screen-overlay"></div>

  <script type="text/javascript">
    /*
    * Replace all SVG images with inline SVG
    */
    jQuery('img.svg').each(function(){
      var $img = jQuery(this);
      var imgID = $img.attr('id');
      var imgClass = $img.attr('class');
      var imgURL = $img.attr('src');

      jQuery.get(imgURL, function(data) {
          // Get the SVG tag, ignore the rest
          var $svg = jQuery(data).find('svg');

          // Add replaced image's ID to the new SVG
          if(typeof imgID !== 'undefined') {
              $svg = $svg.attr('id', imgID);
          }
          // Add replaced image's classes to the new SVG
          if(typeof imgClass !== 'undefined') {
              $svg = $svg.attr('class', imgClass+' replaced-svg');
          }

          // Remove any invalid XML tags as per http://validator.w3.org
          $svg = $svg.removeAttr('xmlns:a');

          // Check if the viewport is set, if the viewport is not set the SVG wont't scale.
          if(!$svg.attr('viewBox') && $svg.attr('height') && $svg.attr('width')) {
              $svg.attr('viewBox', '0 0 ' + $svg.attr('height') + ' ' + $svg.attr('width'))
          }

          $svg.attr('preserveAspectRatio', 'xMinYMin');

          // Replace image with new SVG
          $img.replaceWith($svg);

      }, 'xml');
    });

    $(document).ready(function() {
      $('#screen-overlay').addClass('hidden');
    });
  </script>

</body>

</html>
